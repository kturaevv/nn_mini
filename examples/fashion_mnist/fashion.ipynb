{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d222119-73f9-4dac-ba1e-2e598d95e8cb",
   "metadata": {},
   "source": [
    "# Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27cf7d8-ab27-4cc6-aaf2-6ff35e8a2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1f43a1-cab6-4d63-bef1-83d1e9ea23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# shuffle train data to diversify sequential input\n",
    "keys = np.array(range(x_train.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "x_train = x_train[keys]\n",
    "y_train = y_train[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5822a196-6a8e-4d44-9881-a725c4689059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten for dense NN, from -1 to 1\n",
    "x_train = (x_train.reshape(x_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "x_test = (x_test.reshape(x_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "assert -1 <= x_train.min(), x_train.max() <= 1 # Ensuring that scaling is correct\n",
    "assert -1 <= x_test.min(), x_test.max() <= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a9815f-cd2c-4b03-9a77-8926613c450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c0124-e39e-4a8f-9b6a-fff074fdf911",
   "metadata": {},
   "source": [
    "# Training custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b8f166-1ac7-4c40-86d1-555340cc99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # path lead to root dir to import module\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680ba54d-ba77-4e13-a8b6-7aed695b89fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 10.817 (data_loss: 10.817, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.836, loss: 1.196 (data_loss: 1.196, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.820, loss: 1.420 (data_loss: 1.420, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.820, loss: 1.084 (data_loss: 1.084, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.766, loss: 1.349 (data_loss: 1.349, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.823, loss: 1.313 (data_loss: 1.313, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.819, loss: 1.287 (data_loss: 1.287, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 1.429 (data_loss: 1.429, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.867, loss: 1.023 (data_loss: 1.023, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.844, loss: 1.201 (data_loss: 1.201, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.867, loss: 1.107 (data_loss: 1.107, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.797, loss: 1.085 (data_loss: 1.085, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.823, loss: 0.990 (data_loss: 0.990, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.865, loss: 1.021 (data_loss: 1.021, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.844, loss: 1.059 (data_loss: 1.059, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.867, loss: 1.074 (data_loss: 1.074, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.844, loss: 1.120 (data_loss: 1.120, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.859, loss: 1.138 (data_loss: 1.138, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.820, loss: 0.927 (data_loss: 0.927, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.833, loss: 0.941 (data_loss: 0.941, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.876, loss: 0.963 (data_loss: 0.963, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.852, loss: 1.041 (data_loss: 1.041, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.891, loss: 0.931 (data_loss: 0.931, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.844, loss: 1.287 (data_loss: 1.287, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.867, loss: 1.022 (data_loss: 1.022, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.836, loss: 0.891 (data_loss: 0.891, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.833, loss: 0.936 (data_loss: 0.936, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.884, loss: 0.920 (data_loss: 0.920, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.859, loss: 1.100 (data_loss: 1.100, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.891, loss: 0.826 (data_loss: 0.826, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.844, loss: 1.282 (data_loss: 1.282, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.867, loss: 1.031 (data_loss: 1.031, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.852, loss: 0.795 (data_loss: 0.795, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.833, loss: 0.915 (data_loss: 0.915, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.889, loss: 0.887 (data_loss: 0.887, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.874, loss: 1.050\n"
     ]
    }
   ],
   "source": [
    "model_manual = nn.model.Model()\n",
    "model_manual.add(nn.layer.Dense(x_train.shape[1], 128, activation=nn.activation.ReLU()))\n",
    "model_manual.add(nn.layer.Dense(128, 10, activation=nn.activation.Softmax()))\n",
    "\n",
    "model_manual.set(\n",
    "    loss = nn.loss.CategoricalCrossentropy(),\n",
    "    accuracy = nn.accuracy.Categorical(),\n",
    "    optimizer = nn.optimizer.Adam(learning_rate=learning_rate, decay=5e-5)\n",
    ")\n",
    "\n",
    "model_manual.train(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs , batch_size = batch_size , print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3244a845-5278-4b30-ac1e-7705c80fd395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.188, loss: 9.169 (data_loss: 9.169, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.844, loss: 1.335 (data_loss: 1.335, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.820, loss: 1.476 (data_loss: 1.476, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.805, loss: 1.130 (data_loss: 1.130, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.750, loss: 1.229 (data_loss: 1.229, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.823, loss: 1.550 (data_loss: 1.550, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.819, loss: 1.162 (data_loss: 1.162, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 1.314 (data_loss: 1.314, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.844, loss: 0.910 (data_loss: 0.910, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.844, loss: 1.184 (data_loss: 1.184, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.859, loss: 0.974 (data_loss: 0.974, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.773, loss: 1.274 (data_loss: 1.274, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.844, loss: 1.245 (data_loss: 1.245, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.865, loss: 0.974 (data_loss: 0.974, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.852, loss: 1.132 (data_loss: 1.132, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.875, loss: 0.930 (data_loss: 0.930, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.852, loss: 1.067 (data_loss: 1.067, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.875, loss: 0.939 (data_loss: 0.939, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.789, loss: 1.118 (data_loss: 1.118, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.833, loss: 1.358 (data_loss: 1.358, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.877, loss: 0.918 (data_loss: 0.918, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.844, loss: 1.089 (data_loss: 1.089, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.891, loss: 0.738 (data_loss: 0.738, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.852, loss: 1.037 (data_loss: 1.037, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.852, loss: 1.003 (data_loss: 1.003, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.805, loss: 1.076 (data_loss: 1.076, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.823, loss: 1.214 (data_loss: 1.214, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.884, loss: 0.887 (data_loss: 0.887, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.859, loss: 1.072 (data_loss: 1.072, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.906, loss: 0.691 (data_loss: 0.691, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.852, loss: 0.958 (data_loss: 0.958, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.883, loss: 0.985 (data_loss: 0.985, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.820, loss: 1.057 (data_loss: 1.057, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.833, loss: 1.087 (data_loss: 1.087, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.890, loss: 0.855 (data_loss: 0.855, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.875, loss: 0.970\n"
     ]
    }
   ],
   "source": [
    "model = nn.model.Model('basic', [x_train.shape[1], 128, 10])\n",
    "model.train(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs , batch_size = batch_size , print_every = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2489c-6d64-48fb-a1fc-7889335d711f",
   "metadata": {},
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c29b32c-b682-401d-b83f-9b77567a100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5112 - accuracy: 0.8164\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3887 - accuracy: 0.8591\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3490 - accuracy: 0.8733\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3267 - accuracy: 0.8797\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3056 - accuracy: 0.8872\n",
      "313/313 - 0s - loss: 0.3758 - accuracy: 0.8610 - 495ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.8610000014305115\n"
     ]
    }
   ],
   "source": [
    "keras = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=x_train[0].shape),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "keras.compile(optimizer=opt,\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "keras.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "test_loss, test_acc = keras.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ca9c0-1fee-41d1-b0e9-457fccd796b4",
   "metadata": {},
   "source": [
    "# PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833453ce-2563-4ee8-b183-9b0297c8f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3b5e42-124e-4ade-867d-9e74001c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNet(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchNet, self).__init__()\n",
    "        \n",
    "        self.linear_relu_stack = tnn.Sequential(\n",
    "            tnn.Linear(784, 128),\n",
    "            tnn.ReLU(),\n",
    "            tnn.Linear(128, 10),\n",
    "            tnn.Softmax(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f41d5d-9ef7-470f-82d2-5ea47b40ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data.\n",
    "# PyTorch uses custom DataLoaders and Datasets\n",
    "# for better pipeline architechture\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = CustomImageDataset(x_train, y_train)\n",
    "test_data = CustomImageDataset(x_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd4ff31-b97c-4a45-908d-dbceed8246f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305379  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.739008  [ 6400/60000]\n",
      "loss: 1.706601  [12800/60000]\n",
      "loss: 1.764147  [19200/60000]\n",
      "loss: 1.735661  [25600/60000]\n",
      "loss: 1.718757  [32000/60000]\n",
      "loss: 1.707036  [38400/60000]\n",
      "loss: 1.650356  [44800/60000]\n",
      "loss: 1.642801  [51200/60000]\n",
      "loss: 1.560623  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 1.696663 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.736954  [    0/60000]\n",
      "loss: 1.695611  [ 6400/60000]\n",
      "loss: 1.651206  [12800/60000]\n",
      "loss: 1.815212  [19200/60000]\n",
      "loss: 1.585290  [25600/60000]\n",
      "loss: 1.693932  [32000/60000]\n",
      "loss: 1.696980  [38400/60000]\n",
      "loss: 1.682546  [44800/60000]\n",
      "loss: 1.588734  [51200/60000]\n",
      "loss: 1.662499  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 1.677769 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.682691  [    0/60000]\n",
      "loss: 1.694496  [ 6400/60000]\n",
      "loss: 1.660961  [12800/60000]\n",
      "loss: 1.700871  [19200/60000]\n",
      "loss: 1.703589  [25600/60000]\n",
      "loss: 1.708350  [32000/60000]\n",
      "loss: 1.667320  [38400/60000]\n",
      "loss: 1.659667  [44800/60000]\n",
      "loss: 1.631583  [51200/60000]\n",
      "loss: 1.646171  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 1.675394 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.624400  [    0/60000]\n",
      "loss: 1.670465  [ 6400/60000]\n",
      "loss: 1.677238  [12800/60000]\n",
      "loss: 1.592194  [19200/60000]\n",
      "loss: 1.645062  [25600/60000]\n",
      "loss: 1.663795  [32000/60000]\n",
      "loss: 1.636402  [38400/60000]\n",
      "loss: 1.726170  [44800/60000]\n",
      "loss: 1.592671  [51200/60000]\n",
      "loss: 1.710235  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 1.670908 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.666144  [    0/60000]\n",
      "loss: 1.607855  [ 6400/60000]\n",
      "loss: 1.659009  [12800/60000]\n",
      "loss: 1.592532  [19200/60000]\n",
      "loss: 1.661117  [25600/60000]\n",
      "loss: 1.721194  [32000/60000]\n",
      "loss: 1.632511  [38400/60000]\n",
      "loss: 1.566149  [44800/60000]\n",
      "loss: 1.647174  [51200/60000]\n",
      "loss: 1.669204  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 1.664923 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch_model = TorchNet().to('cpu')\n",
    "\n",
    "loss_fn = tnn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, torch_model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, torch_model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9be71ce5a52a3208977f2e7fdd9a07588940f06dcea6fd3e3d7564008a8353e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
