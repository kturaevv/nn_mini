{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d222119-73f9-4dac-ba1e-2e598d95e8cb",
   "metadata": {},
   "source": [
    "# Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27cf7d8-ab27-4cc6-aaf2-6ff35e8a2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1f43a1-cab6-4d63-bef1-83d1e9ea23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# shuffle train data to diversify sequential input\n",
    "keys = np.array(range(x_train.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "x_train = x_train[keys]\n",
    "y_train = y_train[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5822a196-6a8e-4d44-9881-a725c4689059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten for dense NN, from -1 to 1\n",
    "x_train = (x_train.reshape(x_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "x_test = (x_test.reshape(x_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "assert -1 <= x_train.min(), x_train.max() <= 1 # Ensuring that scaling is correct\n",
    "assert -1 <= x_test.min(), x_test.max() <= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a9815f-cd2c-4b03-9a77-8926613c450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c0124-e39e-4a8f-9b6a-fff074fdf911",
   "metadata": {},
   "source": [
    "# Training custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b8f166-1ac7-4c40-86d1-555340cc99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # path lead to root dir to import module\n",
    "\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680ba54d-ba77-4e13-a8b6-7aed695b89fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.117, loss: 8.701 (data_loss: 8.701, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.789, loss: 1.399 (data_loss: 1.399, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.875, loss: 1.213 (data_loss: 1.213, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.844, loss: 1.285 (data_loss: 1.285, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.859, loss: 0.750 (data_loss: 0.750, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.885, loss: 1.428 (data_loss: 1.428, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.819, loss: 1.296 (data_loss: 1.296, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.852, loss: 1.224 (data_loss: 1.224, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.852, loss: 1.171 (data_loss: 1.171, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.906, loss: 1.034 (data_loss: 1.034, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.891, loss: 1.219 (data_loss: 1.219, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.883, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.885, loss: 1.251 (data_loss: 1.251, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.865, loss: 1.060 (data_loss: 1.060, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.891, loss: 1.271 (data_loss: 1.271, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.875, loss: 1.116 (data_loss: 1.116, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.891, loss: 1.104 (data_loss: 1.104, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.883, loss: 1.183 (data_loss: 1.183, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.898, loss: 0.771 (data_loss: 0.771, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.896, loss: 1.316 (data_loss: 1.316, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.876, loss: 0.979 (data_loss: 0.979, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.914, loss: 1.242 (data_loss: 1.242, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.891, loss: 1.080 (data_loss: 1.080, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.898, loss: 1.002 (data_loss: 1.002, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.891, loss: 1.167 (data_loss: 1.167, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.906, loss: 0.664 (data_loss: 0.664, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.896, loss: 1.283 (data_loss: 1.283, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.884, loss: 0.935 (data_loss: 0.935, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.898, loss: 1.136 (data_loss: 1.136, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.891, loss: 1.186 (data_loss: 1.186, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.891, loss: 0.979 (data_loss: 0.979, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.898, loss: 1.156 (data_loss: 1.156, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.898, loss: 0.645 (data_loss: 0.645, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.906, loss: 1.119 (data_loss: 1.119, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.889, loss: 0.896 (data_loss: 0.896, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.869, loss: 1.112\n"
     ]
    }
   ],
   "source": [
    "model_manual = nn.model.Model()\n",
    "model_manual.add(nn.layer.Dense(x_train.shape[1], 128, activation=nn.activation.ReLU()))\n",
    "model_manual.add(nn.layer.Dense(128, 10, activation=nn.activation.Softmax()))\n",
    "\n",
    "model_manual.set(\n",
    "    loss = nn.loss.CategoricalCrossentropy(),\n",
    "    accuracy = nn.accuracy.Categorical(),\n",
    "    optimizer = nn.optimizer.Adam(learning_rate=learning_rate, decay=5e-5)\n",
    ")\n",
    "\n",
    "model_manual.train(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs , batch_size = batch_size , print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3244a845-5278-4b30-ac1e-7705c80fd395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.102, loss: 10.775 (data_loss: 10.775, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.766, loss: 1.293 (data_loss: 1.293, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.883, loss: 1.136 (data_loss: 1.136, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.867, loss: 1.386 (data_loss: 1.386, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.852, loss: 0.889 (data_loss: 0.889, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.875, loss: 1.340 (data_loss: 1.340, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.821, loss: 1.377 (data_loss: 1.377, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.844, loss: 1.054 (data_loss: 1.054, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.852, loss: 1.089 (data_loss: 1.089, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.891, loss: 0.953 (data_loss: 0.953, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.891, loss: 1.165 (data_loss: 1.165, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.875, loss: 0.721 (data_loss: 0.721, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.896, loss: 1.351 (data_loss: 1.351, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.865, loss: 1.096 (data_loss: 1.096, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.883, loss: 1.180 (data_loss: 1.180, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.875, loss: 1.059 (data_loss: 1.059, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.898, loss: 0.882 (data_loss: 0.882, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.914, loss: 1.185 (data_loss: 1.185, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.883, loss: 0.690 (data_loss: 0.690, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.896, loss: 1.336 (data_loss: 1.336, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.876, loss: 1.022 (data_loss: 1.022, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.883, loss: 1.160 (data_loss: 1.160, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.875, loss: 1.145 (data_loss: 1.145, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.906, loss: 0.872 (data_loss: 0.872, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.906, loss: 1.067 (data_loss: 1.067, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.891, loss: 0.665 (data_loss: 0.665, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.896, loss: 1.435 (data_loss: 1.435, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.883, loss: 0.968 (data_loss: 0.968, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.898, loss: 1.240 (data_loss: 1.240, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.883, loss: 1.112 (data_loss: 1.112, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.914, loss: 0.767 (data_loss: 0.767, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.906, loss: 1.052 (data_loss: 1.052, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.891, loss: 0.647 (data_loss: 0.647, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.896, loss: 1.421 (data_loss: 1.421, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.887, loss: 0.929 (data_loss: 0.929, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.867, loss: 1.142\n"
     ]
    }
   ],
   "source": [
    "model = nn.model.Model('basic', [x_train.shape[1], 128, 10])\n",
    "model.train(x_train, y_train, validation_data = (x_test, y_test), epochs = epochs , batch_size = batch_size , print_every = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2489c-6d64-48fb-a1fc-7889335d711f",
   "metadata": {},
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c29b32c-b682-401d-b83f-9b77567a100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5168 - accuracy: 0.8144\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3832 - accuracy: 0.8622\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3471 - accuracy: 0.8738\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3228 - accuracy: 0.8832\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3022 - accuracy: 0.8894\n",
      "313/313 - 0s - loss: 0.3900 - accuracy: 0.8639 - 461ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.8639000058174133\n"
     ]
    }
   ],
   "source": [
    "keras = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=x_train[0].shape),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "keras.compile(optimizer=opt,\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "keras.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "test_loss, test_acc = keras.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ca9c0-1fee-41d1-b0e9-457fccd796b4",
   "metadata": {},
   "source": [
    "# PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833453ce-2563-4ee8-b183-9b0297c8f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3b5e42-124e-4ade-867d-9e74001c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchNet(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchNet, self).__init__()\n",
    "        \n",
    "        self.linear_relu_stack = tnn.Sequential(\n",
    "            tnn.Linear(784, 128),\n",
    "            tnn.ReLU(),\n",
    "            tnn.Linear(128, 10),\n",
    "            tnn.Softmax(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f41d5d-9ef7-470f-82d2-5ea47b40ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data.\n",
    "# PyTorch uses custom DataLoaders and Datasets\n",
    "# for better pipeline architechture\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = CustomImageDataset(x_train, y_train)\n",
    "test_data = CustomImageDataset(x_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd4ff31-b97c-4a45-908d-dbceed8246f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301408  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.733251  [ 6400/60000]\n",
      "loss: 1.676340  [12800/60000]\n",
      "loss: 1.672975  [19200/60000]\n",
      "loss: 1.740534  [25600/60000]\n",
      "loss: 1.604142  [32000/60000]\n",
      "loss: 1.643003  [38400/60000]\n",
      "loss: 1.596713  [44800/60000]\n",
      "loss: 1.638399  [51200/60000]\n",
      "loss: 1.673391  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.640613 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.704223  [    0/60000]\n",
      "loss: 1.640135  [ 6400/60000]\n",
      "loss: 1.631602  [12800/60000]\n",
      "loss: 1.580439  [19200/60000]\n",
      "loss: 1.640736  [25600/60000]\n",
      "loss: 1.590071  [32000/60000]\n",
      "loss: 1.578810  [38400/60000]\n",
      "loss: 1.646479  [44800/60000]\n",
      "loss: 1.597729  [51200/60000]\n",
      "loss: 1.618792  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 1.630586 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.648543  [    0/60000]\n",
      "loss: 1.654793  [ 6400/60000]\n",
      "loss: 1.551736  [12800/60000]\n",
      "loss: 1.609875  [19200/60000]\n",
      "loss: 1.593598  [25600/60000]\n",
      "loss: 1.658862  [32000/60000]\n",
      "loss: 1.587372  [38400/60000]\n",
      "loss: 1.674248  [44800/60000]\n",
      "loss: 1.660000  [51200/60000]\n",
      "loss: 1.654252  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 1.619046 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.614358  [    0/60000]\n",
      "loss: 1.613827  [ 6400/60000]\n",
      "loss: 1.572110  [12800/60000]\n",
      "loss: 1.575628  [19200/60000]\n",
      "loss: 1.656683  [25600/60000]\n",
      "loss: 1.525677  [32000/60000]\n",
      "loss: 1.558784  [38400/60000]\n",
      "loss: 1.552248  [44800/60000]\n",
      "loss: 1.617241  [51200/60000]\n",
      "loss: 1.576946  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 1.619747 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.634356  [    0/60000]\n",
      "loss: 1.597775  [ 6400/60000]\n",
      "loss: 1.607130  [12800/60000]\n",
      "loss: 1.583063  [19200/60000]\n",
      "loss: 1.651792  [25600/60000]\n",
      "loss: 1.584264  [32000/60000]\n",
      "loss: 1.637568  [38400/60000]\n",
      "loss: 1.638372  [44800/60000]\n",
      "loss: 1.631440  [51200/60000]\n",
      "loss: 1.653788  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 1.608574 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch_model = TorchNet().to('cpu')\n",
    "\n",
    "loss_fn = tnn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, torch_model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, torch_model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9be71ce5a52a3208977f2e7fdd9a07588940f06dcea6fd3e3d7564008a8353e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
