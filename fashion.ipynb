{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97bed35-e985-4d82-a015-48c5c1d29202",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7704c797-b6bb-4a72-b2af-0b99b5f1ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'examples/fashion_mnist/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa692d57-6c65-449b-a879-dd6f0cd72593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a2e87-31de-414f-9c43-00919c696e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILE):\n",
    "    print(f\"Downloading {URL} and saving as {FILE}...\")\n",
    "    urllib.request.urlretrieve(URL,FILE)\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    print(\"Unzipping images...\")\n",
    "    with ZipFile(FILE) as zip_images:\n",
    "        zip_images.extractall(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a15b88-a9a6-40a3-bb9e-753700e8084c",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb71f0e6-ad90-437b-8521-903212e5f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR = 'examples/fashion_mnist'\n",
    "DATA_DIR = 'examples/fashion_mnist/data'\n",
    "TRAIN_DIR = 'examples/fashion_mnist/data/train'\n",
    "TEST_DIR = 'examples/fashion_mnist/data/test'\n",
    "\n",
    "labels = os.listdir(TRAIN_DIR)\n",
    "\n",
    "files = os.listdir(TRAIN_DIR + '/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d226c5-a740-446b-ab06-7bc54197a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image_data = cv2.imread(TRAIN_DIR + '/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011e32a3-476e-4a72-ac21-b8f6694a2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset,path):\n",
    "    \n",
    "    # Scan ll the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "            \n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c0124-e39e-4a8f-9b6a-fff074fdf911",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e336e3-af3e-46c0-ac5e-95c50211eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_test,y_test = create_data_mnist(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330b31de-ea2a-4039-b509-69345c13fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6e6e2-a8e4-44f3-bdc0-a7020133648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten for dense NN, from -1 to 1\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) -\n",
    "127.5 ) / 127.5\n",
    "\n",
    "assert -1 <= X.min(), X.max() <= 1 # Ensuring that scaling is correct\n",
    "assert -1 <= X_test.min(), X_test.max() <= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b8f166-1ac7-4c40-86d1-555340cc99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.094, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.758, loss: 0.720 (data_loss: 0.720, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.758, loss: 0.570 (data_loss: 0.570, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.781, loss: 0.566 (data_loss: 0.566, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.852, loss: 0.405 (data_loss: 0.405, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.854, loss: 0.427 (data_loss: 0.427, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.731, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.773, loss: 0.528 (data_loss: 0.528, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.805, loss: 0.440 (data_loss: 0.440, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.891, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.867, loss: 0.413 (data_loss: 0.413, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.875, loss: 0.356 (data_loss: 0.356, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.854, loss: 0.361 (data_loss: 0.361, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.834, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.789, loss: 0.483 (data_loss: 0.483, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.828, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.883, loss: 0.303 (data_loss: 0.303, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.883, loss: 0.373 (data_loss: 0.373, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.891, loss: 0.322 (data_loss: 0.322, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.865, loss: 0.331 (data_loss: 0.331, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.853, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.797, loss: 0.475 (data_loss: 0.475, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.852, loss: 0.357 (data_loss: 0.357, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.875, loss: 0.286 (data_loss: 0.286, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.891, loss: 0.358 (data_loss: 0.358, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.883, loss: 0.304 (data_loss: 0.304, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.865, loss: 0.311 (data_loss: 0.311, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.863, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.828, loss: 0.448 (data_loss: 0.448, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.859, loss: 0.339 (data_loss: 0.339, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.883, loss: 0.278 (data_loss: 0.278, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.898, loss: 0.344 (data_loss: 0.344, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.898, loss: 0.293 (data_loss: 0.293, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.885, loss: 0.289 (data_loss: 0.289, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.871, loss: 0.356 (data_loss: 0.356, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.859, loss: 0.394\n"
     ]
    }
   ],
   "source": [
    "from nn.model import *\n",
    "\n",
    "from nn.layer import *\n",
    "from nn.activation import *\n",
    "\n",
    "from nn.optimizer import *\n",
    "from nn.loss import *\n",
    "from nn.accuracy import *\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 10 ))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    optimizer = Optimizer_Adam( decay = 5e-5 ),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test), \n",
    "            epochs = 5 , batch_size = 128 , print_every = 100)\n",
    "\n",
    "# Retrieve model parameters\n",
    "parameters = model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3244a845-5278-4b30-ac1e-7705c80fd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New model\n",
    "# # Instantiate the model\n",
    "# model = Model()\n",
    "# # Add layers\n",
    "# model.add(Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "# model.add(Activation_ReLU())\n",
    "# model.add(Layer_Dense( 128 , 128 ))\n",
    "# model.add(Activation_ReLU())\n",
    "# model.add(Layer_Dense( 128 , 10 ))\n",
    "# model.add(Activation_Softmax())\n",
    "# # Set loss and accuracy objects\n",
    "# # We do not set optimizer object this time - there's no need to do it\n",
    "# # as we won't train the model\n",
    "# model.set(\n",
    "# loss = Loss_CategoricalCrossentropy(),\n",
    "# accuracy = Accuracy_Categorical()\n",
    "# )\n",
    "# # Set model with parameters instead of training it\n",
    "# model.set_parameters(parameters)\n",
    "# # Evaluate the model\n",
    "# model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ab8b59c-6c2f-4953-a86a-4a88dba08e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(DIR + '/params'):\n",
    "    model.save_parameters(DIR + '/params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f81721f-f95d-42c7-a2d9-ec29a39817c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.859, loss: 0.394\n"
     ]
    }
   ],
   "source": [
    "model = Model('basic', [X.shape[1], 128, 10])\n",
    "model.set_parameters(parameters)\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9be71ce5a52a3208977f2e7fdd9a07588940f06dcea6fd3e3d7564008a8353e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
