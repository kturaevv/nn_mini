{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97bed35-e985-4d82-a015-48c5c1d29202",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7704c797-b6bb-4a72-b2af-0b99b5f1ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'examples/fashion_mnist/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa692d57-6c65-449b-a879-dd6f0cd72593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a2e87-31de-414f-9c43-00919c696e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILE):\n",
    "    print(f\"Downloading {URL} and saving as {FILE}...\")\n",
    "    urllib.request.urlretrieve(URL,FILE)\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    print(\"Unzipping images...\")\n",
    "    with ZipFile(FILE) as zip_images:\n",
    "        zip_images.extractall(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a15b88-a9a6-40a3-bb9e-753700e8084c",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb71f0e6-ad90-437b-8521-903212e5f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = 'examples/fashion_mnist/data'\n",
    "TRAIN_DIR = 'examples/fashion_mnist/data/train'\n",
    "TEST_DIR = 'examples/fashion_mnist/data/test'\n",
    "\n",
    "labels = os.listdir(TRAIN_DIR)\n",
    "\n",
    "files = os.listdir(TRAIN_DIR + '/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d226c5-a740-446b-ab06-7bc54197a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image_data = cv2.imread(TRAIN_DIR + '/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011e32a3-476e-4a72-ac21-b8f6694a2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset,path):\n",
    "    \n",
    "    # Scan ll the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "            \n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c0124-e39e-4a8f-9b6a-fff074fdf911",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e336e3-af3e-46c0-ac5e-95c50211eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_test,y_test = create_data_mnist(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330b31de-ea2a-4039-b509-69345c13fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6e6e2-a8e4-44f3-bdc0-a7020133648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten for dense NN, from -1 to 1\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) -\n",
    "127.5 ) / 127.5\n",
    "\n",
    "assert -1 <= X.min(), X.max() <= 1 # Ensuring that scaling is correct\n",
    "assert -1 <= X_test.min(), X_test.max() <= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b8f166-1ac7-4c40-86d1-555340cc99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.102, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.742, loss: 0.797 (data_loss: 0.797, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.773, loss: 0.689 (data_loss: 0.689, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.766, loss: 0.596 (data_loss: 0.596, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.875, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.729, loss: 0.725 (data_loss: 0.725, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.716, loss: 0.760 (data_loss: 0.760, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.820, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.805, loss: 0.477 (data_loss: 0.477, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.859, loss: 0.443 (data_loss: 0.443, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.805, loss: 0.454 (data_loss: 0.454, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.883, loss: 0.348 (data_loss: 0.348, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.781, loss: 0.697 (data_loss: 0.697, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.828, loss: 0.472 (data_loss: 0.472, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.852, loss: 0.374 (data_loss: 0.374, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.812, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.859, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.836, loss: 0.412 (data_loss: 0.412, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.914, loss: 0.306 (data_loss: 0.306, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.760, loss: 0.675 (data_loss: 0.675, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.848, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.883, loss: 0.341 (data_loss: 0.341, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.852, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.867, loss: 0.371 (data_loss: 0.371, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.844, loss: 0.394 (data_loss: 0.394, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.914, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.771, loss: 0.669 (data_loss: 0.669, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.858, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.898, loss: 0.327 (data_loss: 0.327, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.875, loss: 0.349 (data_loss: 0.349, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.875, loss: 0.352 (data_loss: 0.352, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.852, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.898, loss: 0.286 (data_loss: 0.286, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.771, loss: 0.660 (data_loss: 0.660, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.866, loss: 0.366 (data_loss: 0.366, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.852, loss: 0.403\n"
     ]
    }
   ],
   "source": [
    "from nn.model import *\n",
    "\n",
    "from nn.layer import *\n",
    "from nn.activation import *\n",
    "\n",
    "from nn.optimizer import *\n",
    "from nn.loss import *\n",
    "from nn.accuracy import *\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 10 ))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    optimizer = Optimizer_Adam( decay = 5e-5 ),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test), \n",
    "            epochs = 5 , batch_size = 128 , print_every = 100)\n",
    "\n",
    "# Retrieve model parameters\n",
    "parameters = model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3244a845-5278-4b30-ac1e-7705c80fd395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.852, loss: 0.403\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 10 ))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss and accuracy objects\n",
    "# We do not set optimizer object this time - there's no need to do it\n",
    "# as we won't train the model\n",
    "model.set(\n",
    "loss = Loss_CategoricalCrossentropy(),\n",
    "accuracy = Accuracy_Categorical()\n",
    ")\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Set model with parameters instead of training it\n",
    "model.set_parameters(parameters)\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a430bf2d-e8e3-4c8c-ba03-ccb75d58ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 128 0 PASS [<nn.layer.Layer_Dense object at 0x000001DC12C18F40>, <nn.activation.Activation_ReLU object at 0x000001DC12C1AD10>]\n",
      "128 128 1 PASS [<nn.layer.Layer_Dense object at 0x000001DC12C18F40>, <nn.activation.Activation_ReLU object at 0x000001DC12C1AD10>, <nn.layer.Layer_Dense object at 0x000001DC12C18E50>, <nn.activation.Activation_ReLU object at 0x000001DC12C1A590>]\n",
      "128 10 2 PASS [<nn.layer.Layer_Dense object at 0x000001DC12C18F40>, <nn.activation.Activation_ReLU object at 0x000001DC12C1AD10>, <nn.layer.Layer_Dense object at 0x000001DC12C18E50>, <nn.activation.Activation_ReLU object at 0x000001DC12C1A590>, <nn.layer.Layer_Dense object at 0x000001DC12C19C60>, <nn.activation.Activation_ReLU object at 0x000001DC12C18E20>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,10) and (128,10) not aligned: 10 (dim 1) != 128 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(problem\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, structure\u001b[38;5;241m=\u001b[39m[X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mset_parameters(parameters)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\neural_networks\\nn\\model.py:293\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, X_val, y_val, batch_size)\u001b[0m\n\u001b[0;32m    288\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m y_val[\n\u001b[0;32m    289\u001b[0m         step\u001b[38;5;241m*\u001b[39mbatch_size:(step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size\n\u001b[0;32m    290\u001b[0m     ]\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Perform the forward pass\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mcalculate(output, batch_y)\n",
      "File \u001b[1;32m~\\Desktop\\neural_networks\\nn\\model.py:222\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Call forward method of every object in a chain\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Pass output of the previous object as a parameter\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# \"layer\" is now the last object from the list,\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# return its output\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[1;32m~\\Desktop\\neural_networks\\nn\\layer.py:24\u001b[0m, in \u001b[0;36mLayer_Dense.forward\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10000,10) and (128,10) not aligned: 10 (dim 1) != 128 (dim 0)"
     ]
    }
   ],
   "source": [
    "from nn.model import Model\n",
    "\n",
    "model = Model(problem='image', structure=[X.shape[1],128, 128, 10])\n",
    "model.set_parameters(parameters)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e37fab-70df-4481-bf17-cc9c93826582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
