{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97bed35-e985-4d82-a015-48c5c1d29202",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7704c797-b6bb-4a72-b2af-0b99b5f1ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'examples/fashion_mnist/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa692d57-6c65-449b-a879-dd6f0cd72593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a2e87-31de-414f-9c43-00919c696e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FILE):\n",
    "    print(f\"Downloading {URL} and saving as {FILE}...\")\n",
    "    urllib.request.urlretrieve(URL,FILE)\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    print(\"Unzipping images...\")\n",
    "    with ZipFile(FILE) as zip_images:\n",
    "        zip_images.extractall(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a15b88-a9a6-40a3-bb9e-753700e8084c",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb71f0e6-ad90-437b-8521-903212e5f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = 'examples/fashion_mnist/data'\n",
    "TRAIN_DIR = 'examples/fashion_mnist/data/train'\n",
    "TEST_DIR = 'examples/fashion_mnist/data/test'\n",
    "\n",
    "labels = os.listdir(TRAIN_DIR)\n",
    "\n",
    "files = os.listdir(TRAIN_DIR + '/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d226c5-a740-446b-ab06-7bc54197a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image_data = cv2.imread(TRAIN_DIR + '/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011e32a3-476e-4a72-ac21-b8f6694a2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset,path):\n",
    "    \n",
    "    # Scan ll the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "            \n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c0124-e39e-4a8f-9b6a-fff074fdf911",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e336e3-af3e-46c0-ac5e-95c50211eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_test,y_test = create_data_mnist(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330b31de-ea2a-4039-b509-69345c13fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6e6e2-a8e4-44f3-bdc0-a7020133648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten for dense NN, from -1 to 1\n",
    "X = (X.reshape(X.shape[ 0 ], - 1 ).astype(np.float32) - 127.5 ) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[ 0 ], - 1 ).astype(np.float32) -\n",
    "127.5 ) / 127.5\n",
    "\n",
    "assert -1 <= X.min(), X.max() <= 1 # Ensuring that scaling is correct\n",
    "assert -1 <= X_test.min(), X_test.max() <= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b8f166-1ac7-4c40-86d1-555340cc99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.711, loss: 0.721 (data_loss: 0.721, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.742, loss: 0.644 (data_loss: 0.644, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.758, loss: 0.683 (data_loss: 0.683, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.797, loss: 0.568 (data_loss: 0.568, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.823, loss: 0.448 (data_loss: 0.448, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.724, loss: 0.736 (data_loss: 0.736, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "epoch: 2\n",
      "step: 0, acc: 0.844, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.852, loss: 0.480 (data_loss: 0.480, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.820, loss: 0.428 (data_loss: 0.428, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.812, loss: 0.531 (data_loss: 0.531, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.820, loss: 0.502 (data_loss: 0.502, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.865, loss: 0.374 (data_loss: 0.374, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.836, loss: 0.453 (data_loss: 0.453, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "epoch: 3\n",
      "step: 0, acc: 0.844, loss: 0.419 (data_loss: 0.419, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.859, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.875, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.844, loss: 0.478 (data_loss: 0.478, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.844, loss: 0.498 (data_loss: 0.498, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.844, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.853, loss: 0.402 (data_loss: 0.402, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "epoch: 4\n",
      "step: 0, acc: 0.844, loss: 0.395 (data_loss: 0.395, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.867, loss: 0.428 (data_loss: 0.428, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.891, loss: 0.351 (data_loss: 0.351, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.844, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.859, loss: 0.486 (data_loss: 0.486, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.875, loss: 0.333 (data_loss: 0.333, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.863, loss: 0.375 (data_loss: 0.375, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "epoch: 5\n",
      "step: 0, acc: 0.828, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.867, loss: 0.408 (data_loss: 0.408, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.875, loss: 0.337 (data_loss: 0.337, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.867, loss: 0.436 (data_loss: 0.436, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.875, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.875, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.870, loss: 0.355 (data_loss: 0.355, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.856, loss: 0.396\n",
      "validation, acc: 0.856, loss: 0.396\n"
     ]
    }
   ],
   "source": [
    "from nn.model import *\n",
    "\n",
    "from nn.layer import *\n",
    "from nn.activation import *\n",
    "\n",
    "from nn.optimizer import *\n",
    "from nn.loss import *\n",
    "from nn.accuracy import *\n",
    "\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 64 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 64 , 10 ))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    optimizer = Optimizer_Adam( decay = 5e-5 ),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data = (X_test, y_test), \n",
    "            epochs = 5 , batch_size = 128 , print_every = 100)\n",
    "\n",
    "# Retrieve model parameters\n",
    "parameters = model.get_parameters()\n",
    "# New model\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[ 1 ], 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 128 ))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense( 128 , 10 ))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss and accuracy objects\n",
    "# We do not set optimizer object this time - there's no need to do it\n",
    "# as we won't train the model\n",
    "model.set(\n",
    "loss = Loss_CategoricalCrossentropy(),\n",
    "accuracy = Accuracy_Categorical()\n",
    ")\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Set model with parameters instead of training it\n",
    "model.set_parameters(parameters)\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aabc618-776b-4930-83dc-85763bad6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8401e5cd-ff3d-48dd-a161-4e21ca6698be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples.fashion_mnist.prms', 'wb') as f:\n",
    "    pickle.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "429fa3a0-d549-44a7-b3cb-af7dc97721a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('examples.fashion_mnist.prms', 'rb') as f:\n",
    "    s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e564d651-60d6-4b20-8d38-af21963de4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array(s)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d41ea-dd26-4eab-b5eb-7be63b8da436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
